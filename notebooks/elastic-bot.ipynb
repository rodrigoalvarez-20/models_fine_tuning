{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import TextConverter, PreProcessor\n",
    "from haystack.nodes import BM25Retriever\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.utils import launch_es\n",
    "from haystack import Pipeline\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializacion del modulo de logging para ver lo que sucede dentro de Haystack\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a nuestro almacen de documentos local\n",
    "# Los documentos en formato txt se almacenarán aqui\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "document_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion del Pipeline, Conversor de texto y Preprocesador\n",
    "# El Pipeline se va a encargar de convertir nuestros archivos en documentos, hacer el split y guardarlos en el document store (elastic)\n",
    "# Es como un pipeline de Jenkins\n",
    "# El Conversor se encarga de convertir los datos dentro del TXT a un Document\n",
    "# El PreProcesador se encarga de limpiar el formato, dividirlo en lineas y generar un formato entendible para el modelo a utilizar\n",
    "indexing_pipeline = Pipeline()\n",
    "text_converter = TextConverter()\n",
    "preprocessor = PreProcessor(\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_overlap=20,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agregan los pasos del pipeline\n",
    "# File --> TextConverter --> Preprocessor --> Document Store\n",
    "indexing_pipeline.add_node(component=text_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
    "indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])\n",
    "indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datasets\n",
    "# Este proceso solo se hace en la sincronizacion inicial o cuando se desee añadir o actualizar un documento\n",
    "doc_dir = \"gi_datasets\"\n",
    "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "indexing_pipeline.run_batch(file_paths=files_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el Retriever para los documentos almacenados\n",
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\n",
    "#deepset/roberta-base-squad2\n",
    "#model_to_use = \"mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n",
    "# Cargamos el modelo a utilizar\n",
    "# \n",
    "model_to_use = \"hy_fine_trained\"\n",
    "# TODO Validar la GPU\n",
    "reader = FARMReader(model_name_or_path=model_to_use, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo pipeline para el proceso de Preguntas y Respuestas\n",
    "# Dado una pregunta, buscara en la base de datos un contexto donde puede contener la respuesta, ayudandose del modelo que hemos indicado\n",
    "# Entre mejor sea el modelo, mas precisa sera la respuesta y mas ambigua puede ser la pregunta\n",
    "# Indagar en lo de GENERALIZACION\n",
    "querying_pipeline = Pipeline()\n",
    "# Query --> Retriever --> Reader \n",
    "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos una pregunta a nuestro pipeline\n",
    "# Los parametros son \"Cuantos documentos del retriever (elastic) quieres que tome como contexto\"\n",
    "# Y \"Cuantos documentos del Reader (salida) quieres dar como resultado\"\n",
    "# Por asi decirlo, limitar en cuantos archivos voy a buscar y cuantas respuestas ofrezco al usuario\n",
    "prediction = querying_pipeline.run(\n",
    "    query=\"¿Qué estudia Laila?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar la clase Answer de Haystack\n",
    "for aw in prediction[\"answers\"]:\n",
    "    print(\"Type: {} - Score: {}\".format(aw.type, aw.score))\n",
    "    print(aw.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATIVE QA System (Plus Elastic Search & Custom Model QA)\n",
    "# Esto es agregado a lo de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "                             Your answer should be in your own words.\n",
    "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0 - Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-small\", default_prompt_template=rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING - haystack.nodes.prompt.invocation_layer.hugging_face -  The prompt has been truncated from 982 tokens to 412 tokens so that the prompt length and answer length (100 tokens) fit within the max token limit (512 tokens). Shorten the prompt to prevent it from being cut off\n"
     ]
    }
   ],
   "source": [
    "#documents = retriever.\n",
    "output = pipe.run(query=\"¿Quien es la Shogun Raiden?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, ltima vez, l\n"
     ]
    }
   ],
   "source": [
    "print(output[\"answers\"][0].answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
